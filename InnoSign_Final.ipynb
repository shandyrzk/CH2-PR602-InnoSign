{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import mediapipe as mp\n",
        "from tensorflow.keras.models import model_from_json\n",
        "from time import sleep\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "metadata": {
        "id": "TgM0eeKLHBBT"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_dPOoOUGm6s"
      },
      "outputs": [],
      "source": [
        "mp_drawing = mp.solutions.drawing_utils\n",
        "mp_drawing_styles = mp.solutions.drawing_styles\n",
        "mp_hands = mp.solutions.hands\n",
        "\n",
        "def mediapipe_detection(image, model):\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    image.flags.writeable = False\n",
        "    results = model.process(image)\n",
        "    image.flags.writeable = True\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "    return image, results\n",
        "\n",
        "def draw_styled_landmarks(image, results):\n",
        "    if results.multi_hand_landmarks:\n",
        "        for hand_landmarks in results.multi_hand_landmarks:\n",
        "            mp_drawing.draw_landmarks(\n",
        "                image,\n",
        "                hand_landmarks,\n",
        "                mp_hands.HAND_CONNECTIONS,\n",
        "                mp_drawing_styles.get_default_hand_landmarks_style(),\n",
        "                mp_drawing_styles.get_default_hand_connections_style())\n",
        "\n",
        "\n",
        "def extract_keypoints(results):\n",
        "    keypoints = []\n",
        "    if results.multi_hand_landmarks:\n",
        "        for hand_landmarks in results.multi_hand_landmarks:\n",
        "            hand = np.array([[res.x, res.y, res.z] for res in hand_landmarks.landmark]).flatten() if hand_landmarks else np.zeros(21*3)\n",
        "            keypoints.append(hand)\n",
        "    return np.concatenate(keypoints) if keypoints else np.zeros(21*3)\n",
        "\n",
        "# Path for exported data, numpy arrays\n",
        "DATA_PATH = os.path.join('MP_Data1')\n",
        "\n",
        "actions = np.array(['A','Absen','akhir','apung','awal','B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'])\n",
        "\n",
        "no_sequences = 30\n",
        "\n",
        "sequence_length = 30"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for action in actions:\n",
        "    for sequence in range(no_sequences):\n",
        "        try:\n",
        "            os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "# cap = cv2.VideoCapture(0)\n",
        "# Set mediapipe model\n",
        "with mp_hands.Hands(\n",
        "    model_complexity=0,\n",
        "    min_detection_confidence=0.5,\n",
        "    min_tracking_confidence=0.5) as hands:\n",
        "\n",
        "    # NEW LOOP\n",
        "    # Loop through actions\n",
        "    for action in actions:\n",
        "        # Loop through sequences aka videos\n",
        "        for sequence in range(no_sequences):\n",
        "            # Loop through video length aka sequence length\n",
        "            for frame_num in range(sequence_length):\n",
        "\n",
        "                # Read feed\n",
        "                # ret, frame = cap.read()\n",
        "                frame=cv2.imread('Image/{}/{}.png'.format(action,sequence))\n",
        "                # frame=cv2.imread('{}{}.png'.format(action,sequence))\n",
        "                # frame=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "                # Make detections\n",
        "                image, results = mediapipe_detection(frame, hands)\n",
        "#                 print(results)\n",
        "\n",
        "                # Draw landmarks\n",
        "                draw_styled_landmarks(image, results)\n",
        "\n",
        "                # NEW Apply wait logic\n",
        "                if frame_num == 0:\n",
        "                    cv2.putText(image, 'STARTING COLLECTION', (120,200),\n",
        "                               cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
        "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12),\n",
        "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
        "                    # Show to screen\n",
        "                    cv2.imshow('OpenCV Feed', image)\n",
        "                    cv2.waitKey(200)\n",
        "                else:\n",
        "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12),\n",
        "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
        "                    # Show to screen\n",
        "                    cv2.imshow('OpenCV Feed', image)\n",
        "\n",
        "                # NEW Export keypoints\n",
        "                keypoints = extract_keypoints(results)\n",
        "                npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
        "                np.save(npy_path, keypoints)\n",
        "\n",
        "                # Break gracefully\n",
        "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
        "                    break\n",
        "\n",
        "    # cap.release()\n",
        "    cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "XV2-AeaWG24c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_hand_landmarks(action, sequence, frame_num):\n",
        "    # Load the keypoints from the .npy file\n",
        "    npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num) + '.npy')\n",
        "    keypoints = np.load(npy_path)\n",
        "\n",
        "    # Extract 3D coordinates from the loaded keypoints for both hands\n",
        "    rh_3d = keypoints[:63].reshape((21, 3))  # Right hand\n",
        "\n",
        "    # Check if landmarks for the left hand exist before attempting to reshape\n",
        "    if len(keypoints) > 63:\n",
        "        lh_3d = keypoints[63:].reshape((21, 3))  # Left hand\n",
        "    else:\n",
        "        lh_3d = None  # No left-hand landmarks\n",
        "\n",
        "    # Create a 3D plot\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "    # Plot right-hand landmarks\n",
        "    ax.scatter(rh_3d[:, 0], rh_3d[:, 1], rh_3d[:, 2], marker='o', s=50, c='r', label='Right Hand Landmarks')\n",
        "\n",
        "    # Plot connections between landmarks for the right hand\n",
        "    connections = mp_hands.HAND_CONNECTIONS\n",
        "    for connection in connections:\n",
        "        start_point = connection[0]\n",
        "        end_point = connection[1]\n",
        "        ax.plot([rh_3d[start_point, 0], rh_3d[end_point, 0]],\n",
        "                [rh_3d[start_point, 1], rh_3d[end_point, 1]],\n",
        "                [rh_3d[start_point, 2], rh_3d[end_point, 2]], c='b')\n",
        "\n",
        "    # Plot left-hand landmarks and connections if they exist\n",
        "    if lh_3d is not None:\n",
        "        ax.scatter(lh_3d[:, 0], lh_3d[:, 1], lh_3d[:, 2], marker='o', s=50, c='g', label='Left Hand Landmarks')\n",
        "        for connection in connections:\n",
        "            start_point = connection[0] + 21\n",
        "            end_point = connection[1] + 21\n",
        "            ax.plot([lh_3d[start_point-21, 0], lh_3d[end_point-21, 0]],\n",
        "                    [lh_3d[start_point-21, 1], lh_3d[end_point-21, 1]],\n",
        "                    [lh_3d[start_point-21, 2], lh_3d[end_point-21, 2]], c='b')\n",
        "\n",
        "    # Set axis labels\n",
        "    ax.set_xlabel('X')\n",
        "    ax.set_ylabel('Y')\n",
        "    ax.set_zlabel('Z')\n",
        "\n",
        "    # Set plot title\n",
        "    ax.set_title(f'3D Hand Landmarks with Connections - Action: {action}, Sequence: {sequence}, Frame: {frame_num}')\n",
        "\n",
        "    # Display the plot\n",
        "    plt.show()\n",
        "\n",
        "action = 'Absen'\n",
        "sequence = 9\n",
        "frame_num = 0\n",
        "\n",
        "visualize_hand_landmarks(action, sequence, frame_num)"
      ],
      "metadata": {
        "id": "tVgSBcRoHyfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "keypoints_per_frame = 63  # 21 keypoints * 3 dimensions\n",
        "DATA_PATH = 'MP_Data1'\n",
        "actions = np.array(['A', 'Absen', 'akhir', 'apung', 'awal', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'])\n",
        "no_sequences = 30\n",
        "sequence_length = 30\n",
        "\n",
        "# Load Data\n",
        "label_map = {label: num for num, label in enumerate(actions)}\n",
        "sequences, labels = [], []\n",
        "for action in actions:\n",
        "    for sequence in range(no_sequences):\n",
        "        window = []\n",
        "        for frame_num in range(sequence_length):\n",
        "            filepath = os.path.join(DATA_PATH, action, str(sequence), f\"{frame_num}.npy\")\n",
        "            if not os.path.exists(filepath):\n",
        "                continue  # Skip if file does not exist\n",
        "            res = np.load(filepath)\n",
        "            if res.shape[0] != keypoints_per_frame:\n",
        "                if res.shape[0] > keypoints_per_frame:\n",
        "                    res = res[:keypoints_per_frame]  # Trim if too long\n",
        "                else:\n",
        "                    res = np.pad(res, (0, keypoints_per_frame - res.shape[0]), 'constant')  # Pad if too short\n",
        "            window.append(res.flatten())\n",
        "        sequences.append(window)\n",
        "        labels.append(label_map[action])\n",
        "\n",
        "# Convert to numpy array and reshape\n",
        "X = np.array(sequences).reshape(-1, sequence_length, keypoints_per_frame, 1)  # Added extra dimension for channels\n",
        "y = to_categorical(labels, num_classes=len(actions))\n",
        "\n",
        "# Train/validation/test split\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.25, random_state=42) # 0.25 x 0.8 = 0.2\n",
        "\n",
        "# Print out the shapes of the datasets\n",
        "X_train.shape, X_val.shape, X_test.shape, y_train.shape, y_val.shape, y_test.shape\n",
        "\n",
        "# Build the model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(sequence_length, keypoints_per_frame, 1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(actions), activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "optimizer = Adam(learning_rate=0.0001)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Early stopping callback\n",
        "# early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train the model with validation and early stopping\n",
        "history = model.fit(X_train, y_train, epochs=200, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "eval_result = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {eval_result[0]}\")\n",
        "print(f\"Test Accuracy: {eval_result[1]}\")\n",
        "\n",
        "# Save the model\n",
        "model.save('final_model.h5')\n",
        "\n",
        "# Save the model architecture as JSON\n",
        "model_json = model.to_json()\n",
        "with open('final_model.json', 'w') as json_file:\n",
        "    json_file.write(model_json)\n",
        "print(\"Model architecture saved as JSON.\")\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nscANIFuG5MT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model architecture and weights\n",
        "json_file = open('final_model.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "model = model_from_json(loaded_model_json)\n",
        "model.load_weights('final_model.h5')\n",
        "\n",
        "# Setup MediaPipe Hands\n",
        "mp_hands = mp.solutions.hands\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "hands = mp_hands.Hands(model_complexity=0, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
        "actions = np.array(['A','Absen','akhir','apung','awal','B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'])\n",
        "# Function to process a single frame's keypoints\n",
        "def process_keypoints(frame, hands_model):\n",
        "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    results = hands_model.process(rgb_frame)\n",
        "    keypoints = np.zeros((21 * 3,))\n",
        "    if results.multi_hand_landmarks:\n",
        "        for hand_landmarks in results.multi_hand_landmarks:\n",
        "            for i, lm in enumerate(hand_landmarks.landmark):\n",
        "                keypoints[i * 3] = lm.x\n",
        "                keypoints[i * 3 + 1] = lm.y\n",
        "                keypoints[i * 3 + 2] = lm.z\n",
        "        return keypoints, True, results.multi_hand_landmarks\n",
        "    return keypoints, False, None\n",
        "\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "keypoints_sequence = []  # To store a sequence of 30 frames' keypoints\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        continue\n",
        "\n",
        "    # Process the current frame to get keypoints and hand landmarks\n",
        "    keypoints, landmarks_detected, hand_landmarks = process_keypoints(frame, hands)\n",
        "\n",
        "    # Draw landmarks and bounding box if landmarks are detected\n",
        "    if landmarks_detected:\n",
        "        # Draw hand landmarks\n",
        "        mp_drawing.draw_landmarks(frame, hand_landmarks[0], mp_hands.HAND_CONNECTIONS)\n",
        "\n",
        "        # Get bounding box coordinates\n",
        "        bbox_coords = []\n",
        "        for lm in hand_landmarks[0].landmark:\n",
        "            h, w, _ = frame.shape\n",
        "            x, y = int(lm.x * w), int(lm.y * h)\n",
        "            bbox_coords.append((x, y))\n",
        "\n",
        "        # Draw bounding box\n",
        "        bbox_coords = np.array(bbox_coords)\n",
        "        bbox = cv2.boundingRect(bbox_coords)\n",
        "        cv2.rectangle(frame, (bbox[0], bbox[1]), (bbox[0] + bbox[2], bbox[1] + bbox[3]), (255, 0, 0), 2)\n",
        "\n",
        "        # Append the processed keypoints to the sequence\n",
        "        keypoints_sequence.append(keypoints)\n",
        "\n",
        "        # Once we have a full sequence, make a prediction\n",
        "        if len(keypoints_sequence) == 30:\n",
        "            keypoints_data = np.array(keypoints_sequence).reshape(-1, 30, 63, 1)\n",
        "            prediction = model.predict(keypoints_data)\n",
        "            gesture_id = np.argmax(prediction)\n",
        "\n",
        "            # Display the prediction on the frame\n",
        "            gesture_label = actions[gesture_id]\n",
        "            cv2.putText(frame, f'Gesture Label: {gesture_label}', (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
        "\n",
        "            # Reset the sequence\n",
        "            keypoints_sequence = []\n",
        "\n",
        "    # Display the frame\n",
        "    cv2.imshow('Hand Gesture Recognition', frame)\n",
        "\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "-J-dEPxCHPky"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}